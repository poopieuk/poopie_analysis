Pipeline Overview

Input:
Paired-end FASTQ files (*_R1.fastq.gz, *_R2.fastq.gz) stored in S3.

Processing Steps:

1. PREPROCESS (R / DADA2)
2. FASTQ detection and QC
3. Filtering & trimming
4. Subsampled learnErrors()
5. Denoising, merging, chimera removal
6. ASV table + taxonomy (SILVA)

Outputs:

- ps_rel.rds
- full_microbiome_summary.json

SUMMARY

Generates per-sample genus abundance CSVs.

BIOMARKERS

Runs biomarker discovery & plots for each sample.

REPORT

1. Uses poopie_report.py + a knowledge base JSON to create a personalised PDF microbiome report.
2. Also outputs a small TXT file for joining channels.

UPLOAD_SUPABASE

Uploads final PDFs (and TXT) directly to a Supabase bucket.

Requirements

1. Nextflow (DSL2)
2. Docker or Singularity images for R + Python environment
3. AWS Batch or local executor
4. Supabase bucket + API key
5. External reference files expected in S3:
6. silva_nr_v138_train_set.fa.gz (taxonomy assignment)
7. silva_species_assignment_v138.fa.gz (taxonomy species assignment)
8. JSON knowledge base (pp_report.json)

Running the Pipeline
nextflow run main.nf \
    --input_dir s3://bucket/raw-data/MYSAMPLE \
    --output_dir results \
    -profile batch


The pipeline automatically detects all paired FASTQs inside input_dir.

Output Structure

results/
 ├── preprocess/
 ├── summary/
 ├── biomarkers/
 ├── pdf/
 ├── supabase_upload/
 └── logs/

Configuration Parameters

Defined in the script:

Parameter	    Description

input_dir	    Path to FASTQs in S3
output_dir	    Local/S3 output path
tax_train	    SILVA training set
tax_species	    SILVA species mapping
preprocess_r	DADA2 pipeline
summary_r	    Summary generator
biomarker_r	    Biomarker R script
report_py	    Python PDF generator
kb_json	        Knowledge base for report
supabase_url, 
supabase_key,   Upload credentials
supabase_bucket	

Override via:

nextflow run main.nf --input_dir s3://... --output_dir ...

Channels & Workflow Structure

FASTQs grouped using Channel.fromFilePairs

PREPROCESS → SUMMARY / BIOMARKERS / REPORT

PDF + TXT are joined → passed to UPLOAD_SUPABASE

Final PDF uploaded to Supabase

AWS Batch Notes

- Works with Tower/Seqera for cloud execution
- Ensure compute environment has:
- On-Demand or Fallback Spot
- Custom Launch Template with large root volume
- m6i/m5/x86 instance family

How Supabase Upload Works

The pipeline:

1. Installs the supabase Python client
2. Authenticates using environment vars
3. Uploads the PDF + TXT to the bucket
4. Writes a success flag upload_done.txt
